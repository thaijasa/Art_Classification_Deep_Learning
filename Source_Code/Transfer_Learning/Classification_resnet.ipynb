{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read X csv file\n",
    "xTrain = pd.read_csv(\"/Users/thaijasa/Documents/Masters/Fall_2018/Large_Scale_Analytics/Project/Dataset_Final/X_train_resnet.csv\")\n",
    "xTrain.drop(['Unnamed: 0'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read Y csv file\n",
    "yTrain = pd.read_csv(\"/Users/thaijasa/Documents/Masters/Fall_2018/Large_Scale_Analytics/Project/Dataset_Final/Y_train_resnet.csv\", header = None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import X test\n",
    "xTest = pd.read_csv(\"/Users/thaijasa/Documents/Masters/Fall_2018/Large_Scale_Analytics/Project/Dataset_Final/X_test_resnet.csv\")\n",
    "xTest.drop(['Unnamed: 0'], axis=1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "yTest = pd.read_csv(\"/Users/thaijasa/Documents/Masters/Fall_2018/Large_Scale_Analytics/Project/Dataset_Final/Y_test_resnet.csv\", header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xTrain shape:  (7721, 100352)\n",
      "yTrain shape:  (7721, 1)\n",
      "xTest shape:  (856, 100352)\n",
      "yTest shape:  (856, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"xTrain shape: \", xTrain.shape)\n",
    "print(\"yTrain shape: \", yTrain.shape)\n",
    "print(\"xTest shape: \", xTest.shape)\n",
    "print(\"yTest shape: \", yTest.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_test, y_train, y_test = train_test_split(sampleX, sampleY, shuffle=True, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:7: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score:  0.9030373831775701\n",
      "Latency Time:  65.25021100000004\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    drawings       0.71      0.65      0.68       122\n",
      "   engraving       0.84      0.67      0.74        84\n",
      " iconography       0.95      0.99      0.97       231\n",
      "    painting       0.95      0.98      0.96       228\n",
      "   sculpture       0.93      0.97      0.95       191\n",
      "\n",
      "   micro avg       0.90      0.90      0.90       856\n",
      "   macro avg       0.87      0.85      0.86       856\n",
      "weighted avg       0.90      0.90      0.90       856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=100)\n",
    "clf = clf.fit(xTrain, yTrain)\n",
    "\n",
    "RF_pred = clf.predict(xTest)\n",
    "\n",
    "stop_time = time.clock()\n",
    "\n",
    "RF_F1 = f1_score(yTest,RF_pred,average = \"micro\")\n",
    "RF_LatencyTime = stop_time - start_time\n",
    "RF_ClassReport = classification_report(yTest,RF_pred)\n",
    "\n",
    "print(\"F1-Score: \",RF_F1)\n",
    "print(\"Latency Time: \", RF_LatencyTime)\n",
    "print(\"Classification Report:\\n \",RF_ClassReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score:  0.7628504672897196\n",
      "Latency Time:  230.57715299999995\n",
      "Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "    drawings       0.43      0.44      0.43       122\n",
      "   engraving       0.49      0.58      0.53        84\n",
      " iconography       0.90      0.85      0.88       231\n",
      "    painting       0.89      0.86      0.87       228\n",
      "   sculpture       0.83      0.83      0.83       191\n",
      "\n",
      "   micro avg       0.76      0.76      0.76       856\n",
      "   macro avg       0.71      0.71      0.71       856\n",
      "weighted avg       0.77      0.76      0.77       856\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#DECISION TREES\n",
    "from sklearn import tree\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "c1f = tree.DecisionTreeClassifier()\n",
    "clf = c1f.fit(xTrain, yTrain)\n",
    "\n",
    "Dectree_pred =c1f.predict(xTest)\n",
    "\n",
    "stop_time = time.clock()\n",
    "\n",
    "Dectree_F1 = f1_score(yTest,Dectree_pred,average = \"micro\")\n",
    "Dectree_LatencyTime = stop_time - start_time\n",
    "Dectree_ClassReport = classification_report(yTest,Dectree_pred)\n",
    "\n",
    "print(\"F1-Score: \",Dectree_F1)\n",
    "print(\"Latency Time: \", Dectree_LatencyTime)\n",
    "print(\"Classification Report:\\n \",Dectree_ClassReport)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:752: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "#GRADIENT BOOST\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "c1f = GradientBoostingClassifier()\n",
    "clf = c1f.fit(xTrain, yTrain)\n",
    "\n",
    "GB_pred =c1f.predict(xTest)\n",
    "\n",
    "stop_time = time.clock()\n",
    "\n",
    "GB_F1 = f1_score(yTest,GB_pred,average = \"micro\")\n",
    "GB_LatencyTime = stop_time - start_time\n",
    "GB_ClassReport = classification_report(yTest,GB_pred)\n",
    "\n",
    "print(\"F1-Score: \",GB_F1)\n",
    "print(\"Latency Time: \", GB_LatencyTime)\n",
    "print(\"Classification Report:\\n \",GB_ClassReport)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "c1f = SGDClassifier()\n",
    "clf = c1f.fit(xTrain, yTrain)\n",
    "\n",
    "SGD_pred =c1f.predict(xTest)\n",
    "\n",
    "stop_time = time.clock()\n",
    "\n",
    "SGD_F1 = f1_score(yTest,SGD_pred,average = \"micro\")\n",
    "SGD_LatencyTime = stop_time - start_time\n",
    "SGD_ClassReport = classification_report(yTest,SGD_pred)\n",
    "\n",
    "print(\"F1-Score: \",SGD_F1)\n",
    "print(\"Latency Time: \", SGD_LatencyTime)\n",
    "print(\"Classification Report:\\n \",SGD_ClassReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KNN\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "c1f = KNeighborsClassifier(n_neighbors=1000, weights='uniform', algorithm='auto')\n",
    "clf = c1f.fit(xTrain, yTrain)\n",
    "\n",
    "KNN_pred =c1f.predict(xTest)\n",
    "\n",
    "stop_time = time.clock()\n",
    "\n",
    "KNN_F1 = f1_score(yTest,KNN_pred,average = \"micro\")\n",
    "KNN_LatencyTime = stop_time - start_time\n",
    "KNN_ClassReport = classification_report(yTest,KNN_pred)\n",
    "\n",
    "print(\"F1-Score: \",KNN_F1)\n",
    "print(\"Latency Time: \", KNN_LatencyTime)\n",
    "print(\"Classification Report:\\n \",KNN_ClassReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extra Tress Classifier\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "clf = ExtraTreesClassifier(n_estimators=2000,n_jobs=-1,criterion='gini',class_weight = 'balanced')\n",
    "clf = clf.fit(xTrain, yTrain)\n",
    "\n",
    "Ext_pred = clf.predict(xTest)\n",
    "\n",
    "stop_time = time.clock()\n",
    "\n",
    "EXT_F1 = f1_score(yTest,Ext_pred,average = \"micro\")\n",
    "EXT_LatencyTime = stop_time - start_time\n",
    "EXT_ClassReport = classification_report(yTest,Ext_pred)\n",
    "\n",
    "print(\"F1-Score: \",EXT_F1)\n",
    "print(\"Latency Time: \", EXT_LatencyTime)\n",
    "print(\"Classification Report:\\n \",EXT_ClassReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adaboost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "clf = AdaBoostClassifier(RandomForestClassifier(n_estimators=500,n_jobs=-1,criterion='gini',class_weight = 'balanced'))\n",
    "clf = clf.fit(xTrain, yTrain)\n",
    "\n",
    "Adab_pred = clf.predict(xTest)\n",
    "\n",
    "stop_time = time.clock()\n",
    "\n",
    "ADB_F1 = f1_score(yTest,Adab_pred,average = \"micro\")\n",
    "ADB_LatencyTime = stop_time - start_time\n",
    "ADB_ClassReport = classification_report(yTest,Adab_pred)\n",
    "\n",
    "print(\"F1-Score: \",ADB_F1)\n",
    "print(\"Latency Time: \", ADB_LatencyTime)\n",
    "print(\"Classification Report:\\n \",ADB_ClassReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "clf = XGBClassifier()\n",
    "clf = clf.fit(xTrain, yTrain)\n",
    "\n",
    "XGB_pred = clf.predict(xTest)\n",
    "\n",
    "stop_time = time.clock()\n",
    "\n",
    "XGB_F1 = f1_score(yTest,XGB_pred,average = \"micro\")\n",
    "XGB_LatencyTime = stop_time - start_time\n",
    "XGB_ClassReport = classification_report(yTest,XGB_pred)\n",
    "\n",
    "print(\"F1-Score: \",XGB_F1)\n",
    "print(\"Latency Time: \", XGB_LatencyTime)\n",
    "print(\"Classification Report:\\n \",XGB_ClassReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SVM SVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "import time\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "clf = SVC(gamma='auto')\n",
    "clf = clf.fit(xTrain, yTrain)\n",
    "\n",
    "SVM_pred = clf.predict(xTest)\n",
    "\n",
    "stop_time = time.clock()\n",
    "\n",
    "SVM_F1 = f1_score(yTest,SVM_pred,average = \"micro\")\n",
    "SVM_LatencyTime = stop_time - start_time\n",
    "SVM_ClassReport = classification_report(yTest,SVM_pred)\n",
    "\n",
    "print(\"F1-Score: \",SVM_F1)\n",
    "print(\"Latency Time: \", SVM_LatencyTime)\n",
    "print(\"Classification Report:\\n \",SVM_ClassReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Naive Bayes Classifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "start_time = time.clock()\n",
    "\n",
    "clf = GaussianNB()\n",
    "clf = clf.fit(xTrain, yTrain)\n",
    "\n",
    "naive_pred = clf.predict(xTest)\n",
    "\n",
    "stop_time = time.clock()\n",
    "\n",
    "naive_F1 = f1_score(yTest,naive_pred,average = \"micro\")\n",
    "naive_LatencyTime = stop_time - start_time\n",
    "naive_ClassReport = classification_report(yTest,naive_pred)\n",
    "\n",
    "print(\"F1-Score: \",naive_F1)\n",
    "print(\"Latency Time: \", naive_LatencyTime)\n",
    "print(\"Classification Report:\\n \",naive_ClassReport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BarChart to compare different F1Scores\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "F1Score =[]\n",
    "label = []\n",
    "\n",
    "F1Score = [RF_F1, Dectree_F1, GB_F1, SGD_F1, KNN_F1, EXT_F1, ADB_F1, XGB_F1, Bag_F1, naive_F1]\n",
    "label = ['RF','DT','GB','SGD','KNN','EXT','ADB','XGB','Baagging','NaiveBayes']\n",
    "\n",
    "x_axis = np.arange(len(F1Score))\n",
    "y_axis = F1Score\n",
    "\n",
    "plt.bar(x_axis, y_axis, color = 'red')\n",
    "plt.xticks(x_axis, label, fontsize=12)\n",
    "\n",
    "plt.xlabel(\"Algorithms\")\n",
    "plt.ylabel('F1Score')\n",
    "plt.title('Comparison of F1 score')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time comparison\n",
    "TimeComparison = [RF_LatencyTime, Dectree_LatencyTime, GB_LatencyTime, SGD_LatencyTime, KNN_LatencyTime, EXT_LatencyTime, ADB_LatencyTime, XGB_LatencyTime, SVM_LatencyTime, naive_LatencyTime] \n",
    "\n",
    "plt.plot( x_axis, TimeComparison, color='purple', linewidth=3, linestyle='dashed', label=\"TimeComparison\")\n",
    "plt.xticks(x_axis, label, fontsize=12)\n",
    "plt.ylabel('Latency Time')\n",
    "plt.xlabel(\"Algorithms\")\n",
    "plt.title('Comparison of Latency Time')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
